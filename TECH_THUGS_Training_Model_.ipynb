{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TECH-THUGS Training Model .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q3Cv2_o3MMM"
      },
      "source": [
        "## EASE THE ERROR 1.0 - SUBMISSION\n",
        "## TEAM : TECH THUGS\n",
        "SSN COLLEGE OF ENGINEERING\n",
        "\n",
        "Members: Vishvambar Panth S (C), Sandhiya S, Saravana Kumar, Dheepika,Rashmika B\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkI8dHlj26fz"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vatWIMZXUIuA"
      },
      "source": [
        "This notebook train a CRNN and stores the weight in drive. This will be used for word prediction in later module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fptLZVibjkQt"
      },
      "source": [
        "import tarfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAtTVE1Yjm_U"
      },
      "source": [
        "tf1= tarfile.open('/content/drive/My Drive/words.tgz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYGtdQswkBId"
      },
      "source": [
        "tf1.extractall('/content/words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3EEXXCnkyfU",
        "outputId": "77f1419a-e733-4f7c-e733-c8dcd4f3cae3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "!pip install keras-tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.activations import relu, sigmoid, softmax\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras_tqdm import TQDMNotebookCallback\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tqdm\n",
            "  Downloading https://files.pythonhosted.org/packages/16/5c/ac63c65b79a895b8994474de2ad4d5b66ac0796b8903d60cfea3f8308d5c/keras_tqdm-2.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-tqdm) (2.4.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tqdm) (4.41.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras->keras-tqdm) (1.15.0)\n",
            "Installing collected packages: keras-tqdm\n",
            "Successfully installed keras-tqdm-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL0zH84hk4jI"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#ignore warnings in the output\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4nqr5X3k5G_",
        "outputId": "3a45f07e-f3eb-4874-a683-7fcaad186b19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "# Check all available devices if GPU is available\n",
        "print(device_lib.list_local_devices())\n",
        "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 8073466014874400654\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 1107611860281006458\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 5822818181713004620\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14640891840\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 5787350994689780254\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cwt-V_7k7ZV",
        "outputId": "3a4e4734-8028-48f3-cc90-7f5f00a9ffcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "tf.config.experimental.list_physical_devices('GPU')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PtNoU4QlAr8",
        "outputId": "cfff9104-48f5-4579-9f0f-2f99e1a32981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('/content/drive/My Drive/words.txt') as f:\n",
        "    contents = f.readlines()\n",
        "\n",
        "lines = [line.strip() for line in contents] \n",
        "lines[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a01-000u-00-00 ok 154 408 768 27 51 AT A'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "endXWXiyl39v",
        "outputId": "9998f230-e46c-434c-a5e6-125123380c32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "max_label_len = 0\n",
        "\n",
        "char_list = \"!\\\"#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\" \n",
        "\n",
        "# string.ascii_letters + string.digits (Chars & Digits)\n",
        "# or \n",
        "# \"!\\\"#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
        "\n",
        "print(char_list, len(char_list))\n",
        "\n",
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    for index, chara in enumerate(txt):\n",
        "        dig_lst.append(char_list.index(chara))\n",
        "        \n",
        "    return dig_lst"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz 78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah6tmUhnl4iR"
      },
      "source": [
        "images = []\n",
        "labels = []\n",
        "\n",
        "RECORDS_COUNT = 10000\n",
        "train_images = []\n",
        "train_labels = []\n",
        "train_input_length = []\n",
        "train_label_length = []\n",
        "train_original_text = []\n",
        "\n",
        "valid_images = []\n",
        "valid_labels = []\n",
        "valid_input_length = []\n",
        "valid_label_length = []\n",
        "valid_original_text = []\n",
        "\n",
        "inputs_length = []\n",
        "labels_length = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxuEh0ODl9db"
      },
      "source": [
        "def process_image(img):\n",
        "    \"\"\"\n",
        "    Converts image to shape (32, 128, 1) & normalize\n",
        "    \"\"\"\n",
        "    w, h = img.shape\n",
        "    \n",
        "#     _, img = cv2.threshold(img, \n",
        "#                            128, \n",
        "#                            255, \n",
        "#                            cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "    \n",
        "    # Aspect Ratio Calculation\n",
        "    new_w = 32\n",
        "    new_h = int(h * (new_w / w))\n",
        "    img = cv2.resize(img, (new_h, new_w))\n",
        "    w, h = img.shape\n",
        "    \n",
        "    img = img.astype('float32')\n",
        "    \n",
        "    # Converts each to (32, 128, 1)\n",
        "    if w < 32:\n",
        "        add_zeros = np.full((32-w, h), 255)\n",
        "        img = np.concatenate((img, add_zeros))\n",
        "        w, h = img.shape\n",
        "    \n",
        "    if h < 128:\n",
        "        add_zeros = np.full((w, 128-h), 255)\n",
        "        img = np.concatenate((img, add_zeros), axis=1)\n",
        "        w, h = img.shape\n",
        "        \n",
        "    if h > 128 or w > 32:\n",
        "        dim = (128,32)\n",
        "        img = cv2.resize(img, dim)\n",
        "    \n",
        "   # img = cv2.subtract(255, img)\n",
        "    \n",
        "    img = np.expand_dims(img, axis=2)\n",
        "    \n",
        "    # Normalize \n",
        "    img = img / 255\n",
        "    \n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I04SHD2amAZ8"
      },
      "source": [
        "for index, line in enumerate(lines):\n",
        "    splits = line.split(' ')\n",
        "    status = splits[1]\n",
        "    \n",
        "    if status == 'ok':\n",
        "        word_id = splits[0]\n",
        "        word = \"\".join(splits[8:])\n",
        "        \n",
        "        splits_id = word_id.split('-')\n",
        "        filepath = 'words/{}/{}-{}/{}.png'.format(splits_id[0], \n",
        "                                                  splits_id[0], \n",
        "                                                  splits_id[1], \n",
        "                                                  word_id)\n",
        "        \n",
        "        # process image\n",
        "        img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "        try:\n",
        "            img = process_image(img)\n",
        "        except:\n",
        "            continue\n",
        "            \n",
        "        # process label\n",
        "        try:\n",
        "            label = encode_to_labels(word)\n",
        "        except:\n",
        "            continue\n",
        "        \n",
        "        if index % 10 == 0:\n",
        "            valid_images.append(img)\n",
        "            valid_labels.append(label)\n",
        "            valid_input_length.append(31)\n",
        "            valid_label_length.append(len(word))\n",
        "            valid_original_text.append(word)\n",
        "        else:\n",
        "            train_images.append(img)\n",
        "            train_labels.append(label)\n",
        "            train_input_length.append(31)\n",
        "            train_label_length.append(len(word))\n",
        "            train_original_text.append(word)\n",
        "        \n",
        "        if len(word) > max_label_len:\n",
        "            max_label_len = len(word)\n",
        "    \n",
        "    if index >= RECORDS_COUNT:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-MaUAu7mDzQ"
      },
      "source": [
        "# padded_label = pad_sequences(labels, \n",
        "#                              maxlen=max_label_len, \n",
        "#                              padding='post',\n",
        "#                              value=len(char_list))\n",
        "\n",
        "train_padded_label = pad_sequences(train_labels, \n",
        "                             maxlen=max_label_len, \n",
        "                             padding='post',\n",
        "                             value=len(char_list))\n",
        "\n",
        "valid_padded_label = pad_sequences(valid_labels, \n",
        "                             maxlen=max_label_len, \n",
        "                             padding='post',\n",
        "                             value=len(char_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyiNewRhmHJX",
        "outputId": "5cbe1dad-6b5f-45fc-94db-4ec186351a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "\n",
        "train_padded_label.shape, valid_padded_label.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7850, 16), (876, 16))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwHlzY_1mJKW"
      },
      "source": [
        "# images = np.asarray(images)\n",
        "# inputs_length = np.asarray(inputs_length)\n",
        "# labels_length = np.asarray(labels_length)\n",
        "\n",
        "train_images = np.asarray(train_images)\n",
        "train_input_length = np.asarray(train_input_length)\n",
        "train_label_length = np.asarray(train_label_length)\n",
        "\n",
        "valid_images = np.asarray(valid_images)\n",
        "valid_input_length = np.asarray(valid_input_length)\n",
        "valid_label_length = np.asarray(valid_label_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-B7Ju5TmLbA",
        "outputId": "fa38cd38-8887-4662-e027-867c78ba2c6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "train_images.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7850, 32, 128, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x94Ef9BDmNQh"
      },
      "source": [
        "# input with shape of height=32 and width=128 \n",
        "inputs = Input(shape=(32,128,1))\n",
        " \n",
        "# convolution layer with kernel size (3,3)\n",
        "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "# poolig layer with kernel size (2,2)\n",
        "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        " \n",
        "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        " \n",
        "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        " \n",
        "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "# poolig layer with kernel size (2,1)\n",
        "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        " \n",
        "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "# Batch normalization layer\n",
        "batch_norm_5 = BatchNormalization()(conv_5)\n",
        " \n",
        "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "batch_norm_6 = BatchNormalization()(conv_6)\n",
        "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        " \n",
        "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        " \n",
        "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        " \n",
        "# bidirectional LSTM layers with units=128\n",
        "blstm_1 = Bidirectional(LSTM(256, return_sequences=True, dropout = 0.2))(squeezed)\n",
        "blstm_2 = Bidirectional(LSTM(256, return_sequences=True, dropout = 0.2))(blstm_1)\n",
        " \n",
        "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
        "\n",
        "# model to be used at test time\n",
        "act_model = Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUqWMaiCmQIM"
      },
      "source": [
        "the_labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
        "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    \n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        "\n",
        "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, the_labels, input_length, label_length])\n",
        "\n",
        "#model to be used at training time\n",
        "model = Model(inputs=[inputs, the_labels, input_length, label_length], outputs=loss_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb0WvnJ4mS1C"
      },
      "source": [
        "batch_size = 8\n",
        "epochs = 30\n",
        "e = str(epochs)\n",
        "optimizer_name = 'sgd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_xd-7UKmUuQ"
      },
      "source": [
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = optimizer_name, metrics=['accuracy'])\n",
        "\n",
        "filepath=\"{}o-{}r-{}e-{}t-{}v.hdf5\".format(optimizer_name,\n",
        "                                          str(RECORDS_COUNT),\n",
        "                                          str(epochs),\n",
        "                                          str(train_images.shape[0]),\n",
        "                                          str(valid_images.shape[0]))\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHEwRAkWmbPr",
        "outputId": "d70ed0c8-04af-4094-9205-467421a726be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "history = model.fit(x=[train_images, train_padded_label, train_input_length, train_label_length],\n",
        "                    y=np.zeros(len(train_images)),\n",
        "                    batch_size=batch_size, \n",
        "                    epochs=epochs, \n",
        "                    validation_data=([valid_images, valid_padded_label, valid_input_length, valid_label_length], [np.zeros(len(valid_images))]),\n",
        "                    verbose=2,\n",
        "                    callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 19.47877, saving model to sgdo-10000r-30e-7850t-876v.hdf5\n",
            "982/982 - 26s - loss: 16.5715 - accuracy: 0.0000e+00 - val_loss: 19.4788 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 00002: val_loss improved from 19.47877 to 14.81419, saving model to sgdo-10000r-30e-7850t-876v.hdf5\n",
            "982/982 - 25s - loss: 14.3945 - accuracy: 0.0071 - val_loss: 14.8142 - val_accuracy: 0.0228\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 14.81419\n",
            "982/982 - 25s - loss: 13.3316 - accuracy: 0.0320 - val_loss: 15.3777 - val_accuracy: 0.0445\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 14.81419\n",
            "982/982 - 25s - loss: 12.1963 - accuracy: 0.0531 - val_loss: 16.7696 - val_accuracy: 0.0023\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 14.81419\n",
            "982/982 - 25s - loss: 11.1974 - accuracy: 0.0657 - val_loss: 38.5415 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 14.81419\n",
            "982/982 - 25s - loss: 10.4113 - accuracy: 0.0819 - val_loss: 17.8990 - val_accuracy: 0.0034\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 00007: val_loss improved from 14.81419 to 11.67894, saving model to sgdo-10000r-30e-7850t-876v.hdf5\n",
            "982/982 - 25s - loss: 9.6544 - accuracy: 0.1023 - val_loss: 11.6789 - val_accuracy: 0.0080\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 11.67894\n",
            "982/982 - 25s - loss: 8.8007 - accuracy: 0.1307 - val_loss: 26.9603 - val_accuracy: 0.0011\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 00009: val_loss improved from 11.67894 to 9.69042, saving model to sgdo-10000r-30e-7850t-876v.hdf5\n",
            "982/982 - 25s - loss: 7.9119 - accuracy: 0.1548 - val_loss: 9.6904 - val_accuracy: 0.1815\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 00010: val_loss improved from 9.69042 to 8.86288, saving model to sgdo-10000r-30e-7850t-876v.hdf5\n",
            "982/982 - 25s - loss: 6.8988 - accuracy: 0.1876 - val_loss: 8.8629 - val_accuracy: 0.1473\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 00011: val_loss improved from 8.86288 to 7.72768, saving model to sgdo-10000r-30e-7850t-876v.hdf5\n",
            "982/982 - 25s - loss: 5.8778 - accuracy: 0.2057 - val_loss: 7.7277 - val_accuracy: 0.1427\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 00012: val_loss improved from 7.72768 to 5.88203, saving model to sgdo-10000r-30e-7850t-876v.hdf5\n",
            "982/982 - 25s - loss: 4.9342 - accuracy: 0.2302 - val_loss: 5.8820 - val_accuracy: 0.2237\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 00013: val_loss improved from 5.88203 to 4.91845, saving model to sgdo-10000r-30e-7850t-876v.hdf5\n",
            "982/982 - 25s - loss: 4.0741 - accuracy: 0.2766 - val_loss: 4.9184 - val_accuracy: 0.2797\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 4.91845\n",
            "982/982 - 25s - loss: 3.4094 - accuracy: 0.3201 - val_loss: 5.4775 - val_accuracy: 0.3059\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 00015: val_loss improved from 4.91845 to 4.43966, saving model to sgdo-10000r-30e-7850t-876v.hdf5\n",
            "982/982 - 25s - loss: 2.8494 - accuracy: 0.3639 - val_loss: 4.4397 - val_accuracy: 0.2945\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 4.43966\n",
            "982/982 - 25s - loss: 2.3815 - accuracy: 0.4082 - val_loss: 7.4066 - val_accuracy: 0.2671\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 00017: val_loss improved from 4.43966 to 3.87789, saving model to sgdo-10000r-30e-7850t-876v.hdf5\n",
            "982/982 - 25s - loss: 2.0058 - accuracy: 0.4464 - val_loss: 3.8779 - val_accuracy: 0.3984\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 00018: val_loss improved from 3.87789 to 3.67134, saving model to sgdo-10000r-30e-7850t-876v.hdf5\n",
            "982/982 - 25s - loss: 1.6292 - accuracy: 0.5001 - val_loss: 3.6713 - val_accuracy: 0.4281\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 00019: val_loss improved from 3.67134 to 3.45989, saving model to sgdo-10000r-30e-7850t-876v.hdf5\n",
            "982/982 - 25s - loss: 1.2992 - accuracy: 0.5526 - val_loss: 3.4599 - val_accuracy: 0.4429\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 3.45989\n",
            "982/982 - 25s - loss: 1.0443 - accuracy: 0.6074 - val_loss: 4.3883 - val_accuracy: 0.4041\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 3.45989\n",
            "982/982 - 24s - loss: 0.8321 - accuracy: 0.6611 - val_loss: 3.6848 - val_accuracy: 0.4760\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 3.45989\n",
            "982/982 - 25s - loss: 0.7280 - accuracy: 0.6955 - val_loss: 3.8646 - val_accuracy: 0.4486\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 3.45989\n",
            "982/982 - 24s - loss: 0.5815 - accuracy: 0.7348 - val_loss: 3.8411 - val_accuracy: 0.4795\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 3.45989\n",
            "982/982 - 25s - loss: 0.4965 - accuracy: 0.7748 - val_loss: 3.6167 - val_accuracy: 0.4989\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 00025: val_loss improved from 3.45989 to 3.45170, saving model to sgdo-10000r-30e-7850t-876v.hdf5\n",
            "982/982 - 25s - loss: 0.3746 - accuracy: 0.8172 - val_loss: 3.4517 - val_accuracy: 0.5377\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 3.45170\n",
            "982/982 - 25s - loss: 0.3134 - accuracy: 0.8488 - val_loss: 3.6710 - val_accuracy: 0.5411\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 3.45170\n",
            "982/982 - 25s - loss: 0.2361 - accuracy: 0.8887 - val_loss: 3.5937 - val_accuracy: 0.5388\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 3.45170\n",
            "982/982 - 25s - loss: 0.1976 - accuracy: 0.9108 - val_loss: 3.5571 - val_accuracy: 0.5457\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 3.45170\n",
            "982/982 - 25s - loss: 0.1585 - accuracy: 0.9276 - val_loss: 3.5594 - val_accuracy: 0.5502\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 3.45170\n",
            "982/982 - 25s - loss: 0.1142 - accuracy: 0.9516 - val_loss: 3.5247 - val_accuracy: 0.5605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdTLJjErmdxM"
      },
      "source": [
        "act_model.load_weights('/content/drive/My Drive/sgdo-10000r-5e-7850t-876v.hdf5')\n",
        " \n",
        "# predict outputs on validation images\n",
        "#prediction = act_model.predict(valid_images[:10])\n",
        " \n",
        "# use CTC decoder\n",
        "#out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                     #    greedy=True)[0][0])\n",
        " \n",
        "# see the results\n",
        "##i = 0\n",
        "#for x in out:\n",
        "#    print(\"original_text =  \", valid_original_text[i])\n",
        "#    print(\"predicted text = \", end = '')\n",
        "#    for p in x:  \n",
        "#        if int(p) != -1:\n",
        "#            print(char_list[int(p)], end = '')       \n",
        "#    print('\\n')\n",
        "#    i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wm5uko_-DcF"
      },
      "source": [
        "img = cv2.imread('/content/drive/My Drive/to predict words/4-4.png', cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "\n",
        "# increase contrast\n",
        "pxmin = np.min(img)\n",
        "pxmax = np.max(img)\n",
        "imgContrast = (img - pxmin) / (pxmax - pxmin) * 255\n",
        "\n",
        "# increase line width\n",
        "kernel = np.ones((3, 3), np.uint8)\n",
        "imgMorph = cv2.erode(imgContrast, kernel, iterations = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZXgnQjoGga9",
        "outputId": "724c52bb-cbc8-4ff4-b695-fb23f9d994a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "plt.imshow(imgMorph)\n",
        "p=process_image(imgMorph)\n",
        "p=p.reshape((1,32,128,1))\n",
        "prediction = act_model.predict(p)\n",
        "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        "for x in out:\n",
        "   # print(\"original_text =  \", valid_original_text[i])\n",
        "    print(\"predicted text = \", end = '')\n",
        "    for p in x:  \n",
        "        if int(p) != -1:\n",
        "            print(char_list[int(p)], end = '')       \n",
        "    print('\\n')\n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted text = for\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADvCAYAAADrXo8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2de4xl11Xmv3XPfdWrq7qqq7urX26/iMnEcdt0PDHJMImZhJCJIEhoRAYxGSmSGQmkRDBDHEaaAcRIiQQERqAgQ0KMBHEgEBJlCGCSICZDsNOx2++0u2233d3uZ1XXu+6t+1jzR90Ovfda3ffUqVuPY38/qVS1d+17zjrn7Lvvues7ay1RVRBCCMkfhc02gBBCSDa4gBNCSE7hAk4IITmFCzghhOQULuCEEJJTuIATQkhOWdMCLiLvEZFjInJCRO7vlVGEEEK6I1mfAxeRBMDzAN4F4DSAbwP4gKo+e63XjI0W9MD+4nW32/b6IhsVslpzAQAJ7LEWJNzWRj8VHx9JfKwA0HSOd1HLQXuhVbFjmqWg3Wrbz+t2O9u5jEkSe+UqSSto9yXLZsxgoeb0db8KLQ33552jrNeyFL0yEee8beBM8a6QuMfb3aa8Rn0Yu533SZpja6dYO7ztZD1v8ZoTrzdAurvoo082LqnqeNx//dX0+twN4ISqvggAIvIQgB8HcM0F/MD+Ir7+1Z3X3Whd7UKw0A5PQiPjAj4k9jL0F5Kg7S2g60l8QRfbLTNmqp2YvqP1fUH7W7O3mDGPT+4N2jOLfWZMvVYyfVkW9ZHhBdN348hU0L5926tmzNsHj9m+ariot5xrMtMOPwy8c9TKOE/GC82gPVwomzF1bZq+9cJ705dgj7cBO3diNnp+94pGtBB6c6KRYjs17T4nGmqX1KxzaUjCeTJQsNupODcIMaN7z7zs9a/FhbIXwKmr2qc7fYQQQjaAdRcxReQ+ETkiIkcuTXoOEkIIIVlYywJ+BsD+q9r7On0BqvqAqh5W1cM7xvjQCyGE9Iq1+MC/DeBWEbkRKwv3TwH4jz2x6nVE7JN8tWX9rceXd5m+/3PpzUH7W8/fZMYMHAuFTXFcpNU0Rjq0o5kzvdOKqEd39QftmV3WB5+I/VY2nhwN2ruT7r5dQl6PZF7AVbUpIj8P4G8BJAA+o6rP9MwyQggh12Utd+BQ1b8G8Nc9soUQQsgqoFOaEEJyypruwDeKJHp0spHPR1ld4udbTzVHzZinl/aZvsdfDfuGnrI+6L1fu5zJJi2H06LVb6dJfSR8fnxuxo5ZXAh93i80TRwCBoo2uOeWyvmwo3LWjFlPv3jiPHe91UjzzLdH/Ex5Xp8Lz0ocpAVkjyvJgvf8+lqCwngHTgghOYULOCGE5BQu4IQQklO4gBNCSE7JhYgZ4wkRafDEqTh5Vi2jqFN1Ew51F0fiRF3HahNmzCOTB01f/XwYJDN42dotCzbTXxqkGQYTefl/ipUwmVJlxgbktCvh/UGraoOUTgzuMH3/NGATc8VUq6eDdr8TpZRVnIqvmycYxuJzWtIkLkpDr8RHL1FWVrLalPVcxti0bOkSXKXBy2S6FeAdOCGE5BQu4IQQklO4gBNCSE7ZUB+4orufzPMbt2L/Uw+fu4993nNOYYBUFKyftJTCvzjVDv3Czy/YxFUnz4+Zvuq50M6+KVtgQOYXu+7fpRkeSyGOpAJQnA8/+ytley+gUfL6ljNmYajf9D01tCdobytaX/6eUhikdGtx3ozJ6reM50QvA8cq0alMU5jBe8/0ym/sVbZ5vVNyEqx5RR62AlvTKkIIIV3hAk4IITmFCzghhOSUNfnAReQkgDkALQBNVT3cC6MIIYR0pxci5jtV9VIPtnNNqlHwQ9YqMp7wE1epXtBsp6RfrYhZiEQ8T7CabocZ+04vjJgxrYv2iAenw7ZXbWezKdZCMag0Z48/cfouzg0G7bNDw2bM1EA4JiktWANSCHQ1Z0h8KnuZrW4osilxqpTHomlawdLLdPdaIQ7CS5sxMp5d5oEIANWozwv+qWZ8g8XBRb3OdEkXCiGE5JS1LuAK4O9E5Dsicl8vDCKEEJKOtbpQ3q6qZ0RkJ4CHReS7qvqPVw/oLOz3AcC+vRmfsSaEEGJY0x24qp7p/L4A4IsA7nbGPKCqh1X18NgYPTaEENIrMt+Bi8gAgIKqznX+fjeAX+uZZdfBy6DmCYR1DaMTPZGnocl122uhKuHpXWxbeeRic1vQvlzrM2PKM1u/xNd60vaiczNGxqXJTreeJbZiQbLozFvARtWSbMSR3Wmio72HJLJGvsZrjrcGxcJq/NDG9ViLC2UXgC/KygkpAvhTVf2bNWyPEELIKsi8gKvqiwDu6KEthBBCVgGd0oQQklNyWZHHI03VFM//2Yp8ZK2Mn2me3zT2b9a0bsZMt8JsfDPzjg98NptPVoeHsr2uGoYftAZsJZ3moFf/ZOOItYq0QSwtEyTT/dz2NhNdGNzUdOYtyYaXybRXFYcq0bbjSl7ANTS2DPtaTRZN3oETQkhO4QJOCCE5hQs4IYTkFC7ghBCSU3IpYnYry7ZViAOJXm1ZMfB8I8y0t7xoxcHB2e7HO7/HBoTMHtjZ9XUecUJGdXSgQgZ1prHN9rWr9thE8nF9e0Ea8dUT53pWUi0jabLq5TU7YpoSdnHZPcAK5CuvyyKiUsQkhJDXPFzACSEkp3ABJ4SQnJILH/hm+/uycroVOopPNXeZMa8sjQZtWbSXpLSQ7fhrY1G1ob2O327CBhdpM/xcL8xYm8rTq//sb5fs/rViAyIKryMfeKyTbFWyVJLpdfWZXpDGv50mICeu5LXS15tEeKVVJDPjHTghhOQULuCEEJJTuIATQkhO6bqAi8hnROSCiDx9Vd+oiDwsIsc7v7evr5mEEEJi0oiYnwXwuwD++Kq++wF8TVU/LiL3d9of7YVBnmAZiwpZMnxdizjzV1Wybb3k2P1iMwzSOdOwn3MX64NBu7BoP1PLC9ky1tW3h9tqjttju+fml0zf9HKYEfHk5KgZs9RvsyZmoTpSM32jA4tBe0d53owZSpaCtieYeWJUNRpWTSGQN8Sefy9oIw216HW1FPv38j66VVsyaIZekJBHmqx+mx1g59kY21RzsgjG7wrv2i7G2S+d89YrEbOn2Qg7RYqnou4fB/Bg5+8HAbw/9R4JIYT0hKw+8F2qerbz9zmslFcjhBCygaxZxFRVxXWC90XkPhE5IiJHJift1xdCCCHZyLqAnxeRCQDo/L5wrYGq+oCqHlbVw2NjfOiFEEJ6RdZIzC8D+CCAj3d+fynNixRbL6pyqBAKVCVH5EhD4mg8062BoH2ytsOMubAQiZjrGJhXHbJRl+8ee8b0TTVDmx6rHDBjTvSHx9L2UhamYPfgnOmb6JsJ2jdX7f3B7mS267azRgIOF8KskZ4YOq9WEP5uI7zex+u7zZizjZFV21NyRNSKkw4yFuBHkkUzJhZ/hwpWRB6QZfu6aH9VJ1q2Gp3vrCXOSsgmBmYtqxhHVXpiZNxXUystx2X+spIUlroP6pDmMcLPAfgWgDeIyGkR+RBWFu53ichxAP+u0yaEELKBdL0DV9UPXONfP9xjWwghhKwCOqUJISSn5CIbYUzvKl9Y0vj20mQrA4DpVn/QPlsbNmNmFsKAmCyVbtIyOrRg+u7tf9H0xed3T+myGfNk3/6gPd+qZLJprGRt2lUKfeC3Vs6ZMeNJ6KddcKSL7HMi3LZ3vafa1t/52NLBoP3NqVvMmJembVBUN0qJPbikYPv6S+Hk2V6xPvDt5bDPO/87SjZwarQY9nn+9fFIlxgqWF961fHnx++5qthji/3paYJ2AHvt3LVDw/tYzwc+165GY6wPvNa2FbfSkETH62ke14J34IQQklO4gBNCSE7hAk4IITmFCzghhOSULSdiVpwsa4uaLRtfLE6koeQEA1QK4XbaTjBSwwkAmolEzNPzNoijNhOKf9WWI854UUIpaHtp7CIutqzwsicSCG8q20CaVvTZX0uzM4fdpWnTN56EwT3jBRuANN0Op+655pAZs6jZhNV+Cfc317aZF2OBGgD+4dIbgvZTp/bYjV9cvU3uNE6c8nTVcO6Wh6yIODwYBokMV20gTxrxc9zJEDlRDq/l7uKMGTOSWNF0POrzhM6hSOhL3GyA9pzE83syCq4D7PWNBUsg+/xOQzV6cqEkLKlGCCGvebiAE0JITuECTgghOWXTfeDTxnXsJZPqTZBOr/CCCEpOgMDZ5TBw59QFW5Gn8mroW/PcX61yRh94iqt7smETbE23Q5/kqcaYGXOiFqaAr6fZmYOX3KkVOX29IKE4KOr8kvWBz9Xt65aWw/O93LB2NxthIEez7iQpWrb3PqWpcFv9k/a6ledWn8ytVXGqDTkxI8sjoU3Ljp6yVA4nWLNlj2Nywfr3JQq26SvZiTpQCn3uewasD/y2ARuU9Ybq2aC9p2gDx0qRDuJpN8eXbfKwr07eHrSfOG91iWZz9Umoys7x95VtFN7OgVArGK9Y7eBAX1gvJ57/14N34IQQklO4gBNCSE7hAk4IITklTT7wz4jIBRF5+qq+XxGRMyJytPPz3vU1kxBCSEwa5emzAH4XwB9H/Z9U1d9Yzc5aakXLZ5dXXw95yKlYEQcDAEDSo+o/cYUQr9JL3cmgFgttcs6KagNn4h5rc7OaTcRMkyDw5WUrYr6MsO/IzA1mzInpcMxiPVsmtnLRikGNVni+l5ftNF2eC/cnC3ZMecaet+JC2Nc/bc93MZpexZodk9Tt9S4uhSJWsmiPTdqrn5PNQaf6y6AV3uZ3h32tij0n86VIoHTE2FQU7fFLKew7M2Szby7stPNkMAmDieJAKsC+lz1h/R9nvs/0/dPxm8J9PWGDdCoZqmAtb7N9C8P22l7YFQ7cs9MGrq2FrldPVf8RwFS3cYQQQjaWtfjAf15Enuy4WOzzcYQQQtaVrAv4pwDcDOAQgLMAfvNaA0XkPhE5IiJHLk9lKxhMCCHEkmkBV9XzqtpS1TaAPwBw93XGPqCqh1X18PZRPvRCCCG9IlP4nIhMqOqV8KmfAPD09cZfQWEzBK4m6mi1DBWyZTGMudS2Wd1iamoFs+cvjwft/lftsQ6dylZDbW5feOkag3b/yyPhN575mlU1/3n6RtNXa4Wi2ekZpxTcbCiGtRvZrmOtYq9RqxYemyxZwa58Odxfedoef+WyFZXKUe218ozdfyGquyVeHS6nK1kK1bBksfu8SUO7bI9f2pt8M+REeWokPi8lVrA8M2/n0mPFUCS/1GejahtRmbPn53eaMc9esJGYydlwzntzIg3xMuVFx7qV0HrzHMU16bqAi8jnALwDwA4ROQ3gfwJ4h4gc6ph3EsDPrqONhBBCHLou4Kr6Aaf70+tgCyGEkFVApzQhhOSUTc9GuJ6caoZ+2pONcTMm9q15LLRDP1rdqc5xqTFo+06OBu29J62TrP8Fm3ktHeGTm7M32EtZXAg/n2fPWt/iY4vWL57Vn71uOH7EwnLogyzPev7udXZAklUzu2QDaR5f3hu0H20dMGPmp8OqOYVp+x4sLlq/dNIbGWzLssXeqYQQQtLCBZwQQnIKF3BCCMkpXMAJISSnbKiI2UQBk+2+7gO7UBUb/DJSsCnFHquFYsiRORu0EgcWNdtW1JxphMLLZG3AjnHEmYGXwm0NHZs0Y1rHTpi+NPQnYea1dtmWJkOURbHglA9rX3SCRJwgjW7EQUMA0B7JFqSUhiSKkfFKlZXmX2epG6LbsXbZnhOJsghqY/3KFbZrdm4tTNo0fqX50IbivLVpe5Q1sm/SE6i7i9bO2ztzts8sFAt2TvYl4fukWkj/vuEdOCGE5BQu4IQQklO4gBNCSE7Z9ECe8eJs0B4p1K4x8l/Yn1g/0u9dPmz6/vzFO4P27CXru05F9DEnzv49tvUml9FrhlKfo10M2epKlxFdp4s2aEMyVFHx0MT6P2vD3YO7Gn32de1ymLypOmV1keLS6oOLaqPWntp2u/+FfeG2Bw7MmjF7toV9l2tWk6o7Wsmy0xfTbIR26hm7bTfp2Ew0xgnKchOK9YiFvaFNTpwemv3h/lvb7Zt7YNTO5b3D4cG9Yfi8GfPG/leD9u7SjBlzLXgHTgghOYULOCGE5BQu4IQQklO6LuAisl9EviEiz4rIMyLy4U7/qIg8LCLHO79ZF5MQQjaQNCJmE8AvqupjIjIE4Dsi8jCA/wzga6r6cRG5H8D9AD66VoOqTlmLp+oTQfvBuVvMmL967g7T1/dUKKKMn88mhDQjLablPPjfyKiPvp7oq1oRc8+gFdouXgiDPfrm7PnuUbEll7i4UmPA7n9xlyN+7gzF7YFTTiUhR6DrxvKwM99s8ku0doTC2h27XjVj7tr2StB+cclm6LxQtxu/XA8ze04tWoFythGOKc9Yu/su2eOvRlVySnPZLm5jqLv4nIb6gbrpG90xF7T3brPz9tbBC6Zvb2U6aN9QvmTG7C+FAX4jhfRPP3S9A1fVs6r6WOfvOQDPAdgL4McBPNgZ9iCA96feKyGEkDWzKh+4iBwEcCeARwDsuqou5jkAu67xmu9VpZ+ZfI0n5yWEkA0k9QIuIoMA/gLAR1Q1+P6gqoprJCK4uir98FhvvuIQQghJuYCLSAkri/efqOpfdrrPi8hE5/8TAKwDiBBCyLqRpiq9YKWI8XOq+ltX/evLAD4I4OOd31/qtq0E7a6RlicbNqveFy/dFbQfOXnQjOl/wooqux8NI6PKr0x1M9GlNRqKOstjdl8Lu2z4Vj2Klmv3l82Y4sTuTDYt7QuFvrl99lIuToRfipbHnPDFohNVGqt4KRgYWzR9B7aH5eL2D9jycc/P7DR9shgeS9FuGojMbhc9oS/bU7K1sfB1tVFnzB57Lsvbw7ndmLZiYGlh9fY4iTaReG+jevgNt9ayc+LtA8eC9puqp8yYF5ftNXmlPha0ny7uMWNm50IRszRnhqA6ZedbdTI8wNJM92hsj/qILRloxzji80To2v2+AzZa8s3bzwTtvRU7l3cXbQRlHGm+O5k3Y0YjRb4q6edtmqdQ3gbgZwA8JSJHO32/jJWF+89E5EMAXgbwH1LvlRBCyJrpuoCr6jcBXOuW7Id7aw4hhJC0MBKTEEJyyoZmI6wIcEPx+oEMT9VtxY7Yl7d+ecl6i0YP3dTGbXa6Uv+E6UvD5VtCf/rMbfYRzf49ob/t5u3WbzdYskELy47vtBtvGrZBI3f2vxy0P3/hLWbMmclh01dYWr0PvmXlBbTK3bcTXyMAiGNb6rudAKQbbHWlwVIYgPFCv43ukk0uEjTXDufgW+JUgABuK9t5crwSXqd6286Rpxb3Be3SglMlacGegKQW+sClme0kqXM7Wt8eds7vszZtPxAe709MPG7G3B5pBUNOsM2QkyJzoBDOwUGxWlki2Zdh3oETQkhO4QJOCCE5hQs4IYTkFC7ghBCSUza9pNpUKxTfplv9ZsxiM1SotL16kWszaEcC2dKYPd1en8E53LkbQzFm4paLZswP7nwpaP/AwEkzZiixZaAauvpp8YNVG/zwvyffGrSfOmuDP5qXnLJbixlETKfEWRq8rH71naEYtWu/FfX+7a4T9nWRsHe8vM+MyRIklZVmPAEBTLdCYXVOrYi5K6mYvlPNUMidctJvymK4v/K8FQyLi1ZsLyz1RsT0iIOw9ICd72+bCN8n9w48b8aMR2JkxREe646K2ogeuZhXK4iv5akM3oETQkhO4QJOCCE5hQs4IYTklA31gbcB1DX0by16kRQZiKvmAMDirtCX16rY6iNpWBwPT1Oc7AgA6jYHFzQKWlrO4Nu9Fq1toS/thiHrp72t72zQ3lOyY6ri+OQyMO24Lb87F6aIr5+3+kZxzrmHiPzEXoBG3Ne28RFoOLmNmgOhoa0xe/wH94VVU96x87gZ8/bBY6bvTy/eE7QTJyApSV9s5Xss29g293jdxGQZeLlpjTy+fGPQPr1gJ3yyFF6UZNna00v/dkzT0UHqO0Of+4Ed02ZMXDUnDXW1QTs1tcfWm3fXteEdOCGE5BQu4IQQklO4gBNCSE7puoCLyH4R+YaIPCsiz4jIhzv9vyIiZ0TkaOfnvetvLiGEkCukETGbAH5RVR8TkSEA3xGRhzv/+6Sq/kbanbVVsdBen1yCzX673cWdUWWVEU/56c7SzlAcqY1bsUKHrKgh86FAW5q1gm3W7HSl4TCL4BuHzpoxt1XCDIHjTtBOVkpR9MHXF28xY16aDqMoSjP2fkFS1LnOqnMvb3cyNO4LMzS+eZfNovhD28NAjh8ZsILlaGKN+vW5sGpN5bIV1arTTnmdLiyN27dpu+y8j5LevLe+XTtg+r41e3PQPnPZZpFMapsbYNfst/sv7gjn/A+MvWLG7CuvvlJXHKCzWaQp6HAWwNnO33Mi8hyAvettGCGEkOuzKh+4iBwEcCeARzpdPy8iT4rIZ0Rk+zVec5+IHBGRI1NOPTxCCCHZSL2Ai8ggVirTf0RVZwF8CsDNAA5h5Q79N73XqeoDqnpYVQ+PjlIzJYSQXpFqRRWRElYW7z9R1b8EAFU9r6otVW0D+AMAd6+fmYQQQmK6+sBFRAB8GsBzqvpbV/VPdPzjAPATAJ7utq1lJDjVctK/XUUC62aJy3719dtIsYUdVlSad8oXdcOL+mvvqgXtPTtt5NbMki2XtjgdhtAVa2YInMNNhbZDQydK1qZqVOJpzg3fs4xG5aLmnOyEzzZ2BO1H5240Yy5Phdd6YDqbyNW0AZzudYop7LDl4t6yJxSx3jf6hBlzVyT+Npx0kMca9pyci673wKQVusoZREwt2H21qs62+8O4v+2VRTOmGl3b4w0rRh5dsCLm01Nh6b+lSRv6PJBVI4+upRazfVP3orEH+8M5sL1kz8lIshC0Y4EeAOZ0a4iWMWmeQnkbgJ8B8JSIHO30/TKAD4jIIawkQzwJ4GfXxUJCCCEuaZ5C+SbcjNT4696bQwghJC1UFQkhJKdsekWemJvKF0zfzHDoBB0oWh/4q8PWl3d5wnGKdaGc2OCPu3acCtpTy7YayT9PHTR9heXoi0tOn6Kca5dN35lG+NToqQXnKdL50Odeciq0pKFVsV8A48Ct1qA9uXtGZ03f4W0ng/btFRsAdbEdZrG82LJpDc81bDa+5Xr4drKzpHe0Rqwv/cYdYbbJO4ZOmTG3l8NMi4/Vd5sxs02r58zXwnMiDS9FZNR0vre3+u2Sk9XnHZNS4jEMFUJxygvSmmmniDhzyGJSIum1It6BE0JITuECTgghOYULOCGE5BQu4IQQklO2nIh50AtIKYQlrW6IhBgAWBiumL6arl5C2F2cMX23RsLqr516nxnTmrVCXyVD+aytiHceLzdDiW62boWvQlRSrLRghqTb/6jta0Wl0Uo7bBTJobEzpu+tfS+E23ZSHZ5qhFkFX17eYcaccWroaWudsvE5m60O2yClN4+Ex/uWvhcz7a7ZtuekHSuSKQR5L9iqVbad7eLqz1u7bF/jZmhMQSOaAyV46S+ziZgxqxEo08A7cEIIySlcwAkhJKdwASeEkJyyoT7wBG2MFLyMTv/CkFg/VqMQ+Tcd13bsxwKAVpqMRxEjiU12c3x5Z9A+u7DNjEnm7P5NIE9GnFxSKJZCn9xYcd4O6hGeD3wmyhzkJfOKK7SUFrJFMonjk9VqePwHoiAWAHjTwGnbF/lOv1mz4Taxz/vFpXEz5pVFG7ikXnBLRLNv9eWFvGpTY0NWUNhfDSvL9LICk3pROd1ek3h+am9kBh+44zf3AnnEWU9i4vk907Y6WDWj77oUHVtN7XugYXrSwztwQgjJKVzACSEkp3ABJ4SQnNJ1AReRqog8KiJPiMgzIvKrnf4bReQRETkhIp8XEde7RQghZH1II2LWAdyrqvOd0mrfFJGvAvgFAJ9U1YdE5PcBfAgrdTKvSVUEt3TJPDaTMWNfSXrzoP26kvH7Tsv5aOyvhoEce4pWxKtmPCex9tTwVNRNptgfZuP7/uFzZsx4cc70/f1SmFnwW/O3mjHfndsVtF+ZtYLl1Ez3XIPNfit81UZXL2I2hqwQN95nRcz4eKuOgFeLxMiS2KyGbUdUbDRCuz2BPp5unu6ZQQt1yfB8AgBg0Xkz1SJl1RMVh6PqXl5AzmLbvrLhVPfpJV1Pg65w5RGHUudHAdwL4Aud/gcBvH9dLCSEEOKStqhx0imndgHAwwBeADCtqlc+vk8D2HuN194nIkdE5MilyRzcJRNCSE5ItYB3qs8fArAPK9Xnb0u7A1V9QFUPq+rhHWOr//pICCHEZ1WeJFWdBvANAPcAGBGRK87RfQBs5iBCCCHrRld1SkTGATRUdVpE+gC8C8AnsLKQ/ySAhwB8EMCXum1LoWj0IKtX4qRCa72Gn4j0Isz6SqH4FGfQA2wWxbTEQleWiNb1plkPv829smBTFv5d+1+ZvuV2OOVPL9isgmcuh+X5li7b0nzFSfvWiXuaTkW/+vbVn8v2sE1rua/fZu0cT8IScl4uzrkU17LessfWboWvk6YV8ZLIzGLdCnhJLdtTCloI9+cFWBYa1qblZngs9bY9tjiKu+VsO1636m0nYtwRLGvaXcT09peWNI8XTAB4UEQSrNyx/5mqfkVEngXwkIj8OoDHAXw6uxmEEEJWS9cFXFWfBHCn0/8iVvzhhBBCNoGt992YEEJIKjY0QqOtisX29X3gVbGfKaOF8DUlSZdlrZHBdztUsPZNt8L9lRPnGBynXJx5rTGYLYqhOdDdSTbXttkA42ozieOjK4mTHS06b16mR7Md55xEbmq0nCoqafD8nbIYTt0XpqwG8JJYv3hcWabVsnNkaS6s7lSYt8ef1LsfS7Pf9i3u7a4BaVRZ5uD+i2aMl2lxdxTIU1vfGJLXFd3WLWBtWQWzwjtwQgjJKVzACSEkp3ABJ4SQnMIFnBBCcsrWSzPnEAub1dRa2OpVnP5CxfT931ZYQm22ZsfE5cMAmzHNK42VhlZ166lRcfbHYmLF0Dhmol3KmIrOOfzyZHhy67PDdlAK2iVn4yM2Q1+v0P7wvA2O2RJ+N49OBu17Rl80Y+7qO2n6hpzMgqQ3bIZAmbXXpmoAAA4LSURBVAbegRNCSE7hAk4IITmFCzghhOSULecDr6n1pcYBCQ2nYkgahpyIkP5CGKThVdV4qT4etOcWbNBMeT6bTY3B0CYvcVW7zwnA8YKJNpBCdC694KZ2FJDSTC9eBJRsYR1UL0VjFrPpBMtD1qbZm8OLEB/HWkiiSkJ37HrVjHnf2BNB+yYnKdmA4+9eTBFwFVMVO98rid12IdI4Ws6u4t23PXOq2e4Z40o+3lzyKldtK4fHt61Yc0wKx8QVqYDsCae8bfUS3oETQkhO4QJOCCE5hQs4IYTklK4LuIhUReRREXlCRJ4RkV/t9H9WRF4SkaOdn0Prby4hhJArpBEx6wDuVdV5ESkB+KaIfLXzv/+mql+4zmtDRJDI9b36LaeCRSxaxln20lJ1hJ96JJouOJU2ZqLSKs26PW196RIkGuKMdZrY/WvFCoRj1YWg7QldB4u2kkuMd77nIsVoKLEHtyNSFkf7bEDKmShoZXmbVyPGEmlKpr3RVPbP92xbh3adC9r3jLxgxtxaPh+0h5wTkCXTpsf+4qzpu7nfZj98aXuY7fEV5z0wn4QqYqPfvk+d5JepiMX9dsXO28r32WN5997vBu0f3faEGTNeqAftqrNGLaSorJMG9x2wBqEzTUEHBXBlBpc6P1svNJAQQl5npPoYF5FERI4CuADgYVV9pPOv/yUiT4rIJ0XExpevvPY+ETkiIkcmJzN+/BJCCDGkWsBVtaWqh7BSff5uEXkTgI8BuA3AWwCMAvjoNV77gKoeVtXDY2PUTAkhpFesakVV1WmsVKN/j6qe1RXqAP4IrI9JCCEbSlcfuIiMA2io6rSI9AF4F4BPiMiEqp4VEQHwfgBPr7Ot3yOrgONFU7VSuPPrUVo9XXZKbC33RhbwDq1QtSLm7mooIu5PrNC2MxkM2ottK2rOe3nWokPxovVGklC0HKssmDHJQPi65oCVcIpW+8wF+7dPm749AzNBe2/VjnlTX1gK7faKjcTsjzI9Zo08ToMXnfz9VWvTS9t2hDa17USd6hsI2kvbXK9qdxwhP4mE/HLZPpDwIzc8Z/r+0/Z/Dtre8cazu9YjwdLDfYhjDftL8xTKBIAHRSTByh37n6nqV0Tk653FXQAcBfBfMltBCCFk1aR5CuVJAHc6/feui0WEEEJSQVWREEJyyoZmIxQApS7+vDQ+6ax4vsRqtD9vTOwDR9OOSWySs0yok/muv9/6rrcVw+Aaz+7Y592A9aV7gTyxp3qoYA9uPAoAOdg3acYcHw2zOJ7Zb6ebLNi+ZCk8lqRuj60YjWksZfMTx4FUANAcC8/bTY6/+00j1k98a18YgPPG6hkzZqwQXrc9jr83zr7p+kidiJgs2pDnk91fstfy9qHQd9+XWF3kwmCouVwedU5uCkoFO0/7o6C04ZKdk/9m2/Omb7wQHt+rrd7csw45NnqkCV3rFtx4PXgHTgghOYULOCGE5BQu4IQQklO4gBNCSE7ZcBGz0MVhn3gi5zqmzjrVCoMNLraGzJiTc2EmtuKMDeSpzGbL8zIf1aZSJ2hnuN9mA+xPumcajEXLOPPiypjueOW7kiQUMe/of8W+cCJsPjWwxwxZjgViAM0oSKTlBI3MN8LMd0vL6TIdxvQV7fm+cySs13Z4+KQZs780Zfp2F0Oxc9zJ4hgHkhjB0mGkYI/fLT3oiNTdKDn3cPuTuum7q+9k0N5dnDFjFtrhe2mubUsPprJJ7HHEwWRxGTQAuLVksyjG56mVcclLokXIm21Vseeysc55/3gHTgghOYULOCGE5BQu4IQQklM22AcuKOH61XTars+oN34kr5LPqUbo336uZv20r85uC9rlGeunL8+t3v8IABL5wL3EVeN9NlHUYIrIodjn7QXtpCFOruT1jVfOmTE3lUJf8juHns20fzeZViHUAObiki1rIK6AU3USIHl7iwMyvOpOU+3VV5Pygka8qjFxUFoaPE1qWMqm7/ZSmHXs1mK2KkVrCVrphhckOPcaLz3DO3BCCMkpXMAJISSncAEnhJCcknoB79TFfFxEvtJp3ygij4jICRH5vIjjOCOEELJurEbE/DCA5wBcUfQ+AeCTqvqQiPw+gA8B+NRaDfJElWqUea3hiGppmG7bCiGnGqNB+4XFcTNmYTYMSNhmNUUU59KExFikFX7uFUv22LxqN16GwJhYtMxmYTo8UW8oCgBKCjawJQ2eiDoQZZkbcQI70oi2aQJpvEpOXrxZLE8uOqK5J6R3o+QE7XjCvidsdqPtnKM6bOBWTH9h9ccBoOtDDGnxMmsutr1smz3ZXSrWO2jHI21V+n0A/j2AP+y0BcC9AL7QGfIgVsqqEUII2SDSulB+G8AvAbhyKzAGYFpVr3xUnwaw13uhiNwnIkdE5MilyWx3zoQQQixdF3AReR+AC6r6nSw7UNUHVPWwqh7eMdabr0+EEELS+cDfBuDHROS9AKpY8YH/DoARESl27sL3AbDlRwghhKwbaYoafwzAxwBARN4B4L+q6k+LyJ8D+EkADwH4IIAvdd0W1BUfuhGLM62sIqbTN98KBcrLdVsGShsb97RluWwFpImqzfw2kiyavpj1FC3TkESaWn+Ga+9tJ/3r7AuzRqPmES8yMRbavKyGaaim8L56ol4d3ffn2d0ti+l6U4ofpPAGpZhbvY5EXcvK9FEAvyAiJ7DiE/90b0wihBCShlXlQlHVfwDwD52/XwRwd+9NIoQQkgZGYhJCSE7Z0GyEbfhVYVZLHMQB+L7N2E/Vcj6vFqNAmpllp4pIcwN94EXHB1623vuhKCim5PgbNzKIwSMO7unlQ6RpgpQ2+/g3mzT+7cw6SYrqTjXN5u8tOX7yqsTtbO/JuLJOVrxj80LrTCZLZ51ai1+cd+CEEJJTuIATQkhO4QJOCCE5hQs4IYTklA0VMVUVtR4EUgw5AkbJEwJ6IJh6eNpMqy/bqYziiNBXsiLmQKHetS8WeYB0mfY84sCZNMXKPFHJBHLkJIgmTeBQmnPilULzMwteH6+kW5bMg4Aj7Ge8JKUNjquJ7fbESC9DYhKd70SzSelZBdn4/Pb6vPEOnBBCcgoXcEIIySlcwAkhJKdsqA98PamIPZTGeqVzcj72WtVsn4XtcugkGygtmzFe4qpqXO3G1QB643P2th0nHHKTDeXE590Nz9+d5px4VXu8gKtupA1a2YyKMFudQQmvXsVJhBcHF/ZCp9soeAdOCCE5hQs4IYTkFC7ghBCSU7iAE0JIThHdQIe9iFwE8DKAHQAubdiOewft3lho98aSR7vzaDOwertvUNXxuHNDF/Dv7VTkiKoe3vAdrxHavbHQ7o0lj3bn0Wagd3bThUIIITmFCzghhOSUzVrAH9ik/a4V2r2x0O6NJY9259FmoEd2b4oPnBBCyNqhC4UQQnLKhi/gIvIeETkmIidE5P6N3n9aROQzInJBRJ6+qm9URB4WkeOd39s308YYEdkvIt8QkWdF5BkR+XCnf6vbXRWRR0XkiY7dv9rpv1FEHunMlc+LSLnbtjYDEUlE5HER+UqnveXtFpGTIvKUiBwVkSOdvi09TwBAREZE5Asi8l0ReU5E7tnqdovIGzrn+crPrIh8pBd2b+gCLiIJgN8D8KMA3gjgAyLyxo20YRV8FsB7or77AXxNVW8F8LVOeyvRBPCLqvpGAG8F8HOd87vV7a4DuFdV7wBwCMB7ROStAD4B4JOqeguAywA+tIk2Xo8PA3juqnZe7H6nqh666nG2rT5PAOB3APyNqt4G4A6snPctbbeqHuuc50MAfgDAIoAvohd2q+qG/QC4B8DfXtX+GICPbaQNq7T3IICnr2ofAzDR+XsCwLHNtrGL/V8C8K482Q2gH8BjAP41VgIdit7c2So/APZ13nz3AvgKVnIQ5sHukwB2RH1bep4AGAbwEjraXV7sjmx9N4D/1yu7N9qFshfAqavapzt9eWGXqp7t/H0OwK7NNOZ6iMhBAHcCeAQ5sLvjhjgK4AKAhwG8AGBaVa/kzd2qc+W3AfwSgCs5SceQD7sVwN+JyHdE5L5O31afJzcCuAjgjzouqz8UkQFsfbuv5qcAfK7z95rtpoiZEV352NySj/CIyCCAvwDwEVWdvfp/W9VuVW3pylfMfQDuBnDbJpvUFRF5H4ALqvqdzbYlA29X1buw4s78ORH5oav/uUXnSRHAXQA+pap3AlhA5HbYonYDADpayI8B+PP4f1nt3ugF/AyA/Ve193X68sJ5EZkAgM7vC5tsj0FESlhZvP9EVf+y073l7b6Cqk4D+AZWXA8jIt+r1LEV58rbAPyYiJwE8BBW3Ci/g61vN1T1TOf3Baz4Y+/G1p8npwGcVtVHOu0vYGVB3+p2X+FHATymquc77TXbvdEL+LcB3NpR6ctY+Trx5Q22YS18GcAHO39/ECs+5i2DiAiATwN4TlV/66p/bXW7x0VkpPN3H1b89s9hZSH/yc6wLWe3qn5MVfep6kGszOWvq+pPY4vbLSIDIjJ05W+s+GWfxhafJ6p6DsApEXlDp+uHATyLLW73VXwA/+I+AXph9yY48d8L4Hms+Dj/+2aLCtex83MAzgJoYOWT/0NY8W9+DcBxAH8PYHSz7YxsfjtWvoY9CeBo5+e9ObD7zQAe79j9NID/0em/CcCjAE5g5WtnZbNtvc4xvAPAV/Jgd8e+Jzo/z1x5H271edKx8RCAI5258lcAtufE7gEAkwCGr+pbs92MxCSEkJxCEZMQQnIKF3BCCMkpXMAJISSncAEnhJCcwgWcEEJyChdwQgjJKVzACSEkp3ABJ4SQnPL/Ab/ThuqNdPS8AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}